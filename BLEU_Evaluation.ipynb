{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "BLEU Evaluation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr7_xzPVAeCy",
        "outputId": "c89e12e0-a3f0-4a30-d101-81a6581d0d3a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixVRbTw9lbyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7699274-5b63-4f9f-cfe1-2d81146ec792"
      },
      "source": [
        "!pip install sentencepiece\r\n",
        "!pip install torch\r\n",
        "!pip install transformers\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3HNNO3gjDN_"
      },
      "source": [
        "#import pretrained models \n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSeq2SeqLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-tw\")\n",
        "#model = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-en-tw\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-tw\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0llaXGTjDOJ"
      },
      "source": [
        "#import further libraries that will be needed\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0VD7RqfjDOK"
      },
      "source": [
        "def translate(input_sentence, model=model, **kwargs):\n",
        "    \"\"\"\n",
        "    Translate the sentence to the target language using the provided model and any keyword arguments provided.\n",
        "    input_sentence: List of sentences\n",
        "    model: model to use for the translation.\n",
        "    \n",
        "    Returns:\n",
        "    List of corresponding translations for the target language\n",
        "    \"\"\"\n",
        "\n",
        "    input_id = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "    beam_output = model.generate(input_id,  **kwargs)\n",
        "    return tokenizer.decode(beam_output[0], skip_special_tokens=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3B-NY_xphQC"
      },
      "source": [
        "def bleuscore_dataframe(dataframe, weights=(0.85,0.15,0,0), smoothFn=None,auto_reweigh=False):\r\n",
        "    \"\"\"\r\n",
        "    compute the bleu score of an entire dataframe dataframe should have a predictions and \r\n",
        "    references column\r\n",
        "    dataframe: pd.DataFrame - should have \"predictions\" and \"references\" columns\r\n",
        "    weights is a 4-tuple summing up to 1. indicates weights for unigrams, bigrams etc\r\n",
        "    smoothFn:int {0,7}. refer to nltk API for details\r\n",
        "    \"\"\"\r\n",
        "    preds = []\r\n",
        "    for line in dataframe['predictions'].values.tolist(): \r\n",
        "        line_clean = line.strip().split()\r\n",
        "        preds.append(line_clean)\r\n",
        "    refs = []\r\n",
        "    for line in dataframe[\"references\"].values.tolist(): \r\n",
        "        line_clean = str(line).strip().split()\r\n",
        "        refs.append(line_clean)\r\n",
        "    refs_2 = [list([(item)]) for item in refs]  # putting it in a form for the nltk bleu method\r\n",
        "    if smoothFn in range(8):\r\n",
        "        smoothFunc = SmoothingFunction()\r\n",
        "        smoothFunc = eval(f\"smoothFunc.method{smoothFn}\")\r\n",
        "    else:\r\n",
        "        smoothFunc = smoothFn\r\n",
        "    BLEUscore = nltk.translate.bleu_score.corpus_bleu(refs_2, preds, weights=weights,\r\n",
        "                                                      smoothing_function=smoothFunc,auto_reweigh=auto_reweigh)\r\n",
        "    return BLEUscore"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u60qbRaojDOK"
      },
      "source": [
        "#read the parrarel dataset to be used for checking scores\n",
        "pairs = pd.read_csv(\"/content/drive/My Drive/GhanaNLP/DATA/GhanaNLP_data_001.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2bP99M3zf-D"
      },
      "source": [
        "def predict_sample_sentences(parallel_sentences, sample_size=100):\r\n",
        "  \"\"\"\r\n",
        "    Takes a dataframe of parrarel eng-twi sentences and translate the english sentences to target language\r\n",
        "\r\n",
        "\r\n",
        "    parallel_sentences: pd.DataFrame - should have \"English\" and \"references\" columns with \"English\" sentences in column index 0 and \"references in col index 1\r\n",
        "    sample_size : number of sentences in the dataframe to translate, default is 100 sentences\r\n",
        "    \r\n",
        "    Return:\r\n",
        "    Returns a dataframe with a third column \"Predictions\" added which contain the translated sentences\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "  test_pairs = parallel_sentences.sample(sample_size)\r\n",
        "  test_pairs.columns= [\"English\", \"references\"]\r\n",
        "  test_pairs[\"predictions\"] = test_pairs[\"English\"].apply(translate)\r\n",
        "  return test_pairs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sxFJQVU1Sb7"
      },
      "source": [
        "#translate some sample sentences\r\n",
        "sample_test_pairs = predict_sample_sentences(pairs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "SVqwL-AH10sh",
        "outputId": "f50916bd-2d2c-4158-8e69-65a26774e281"
      },
      "source": [
        "#take a preview of 6 of the sentences\r\n",
        "sample_test_pairs.sample(6, replace=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>references</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>How are you doing?</td>\n",
              "      <td>Ɛte sɛn?</td>\n",
              "      <td>Ɔkwan bɛn so na woreyɛ saa?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>Yaw is an evil man.</td>\n",
              "      <td>Yaw yɛ nipa bɔne ni/ Yaw tirim wɔ sum.</td>\n",
              "      <td>Yaw yɛ ɔbɔnefo.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>I am serious</td>\n",
              "      <td>m'ani abre</td>\n",
              "      <td>M'ani abere paa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Think about yourself</td>\n",
              "      <td>Dwene wo ho</td>\n",
              "      <td>Susuw wo ho hwɛ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>What items are you travelling with?</td>\n",
              "      <td>Ɛdeɛn nnoɔma na wo de retu kwan no?</td>\n",
              "      <td>Nneɛma bɛn na wode retu kwan?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>I'm not taking any chances.</td>\n",
              "      <td>Mɛyɛ birbiara/ Memma m'ani mpa biribiara a ehi...</td>\n",
              "      <td>Merentumi nyɛ ho hwee.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  English  ...                    predictions\n",
              "139                    How are you doing?  ...    Ɔkwan bɛn so na woreyɛ saa?\n",
              "659                   Yaw is an evil man.  ...                Yaw yɛ ɔbɔnefo.\n",
              "102                          I am serious  ...                M'ani abere paa\n",
              "8                   Think about yourself   ...                Susuw wo ho hwɛ\n",
              "313  What items are you travelling with?   ...  Nneɛma bɛn na wode retu kwan?\n",
              "645           I'm not taking any chances.  ...         Merentumi nyɛ ho hwee.\n",
              "\n",
              "[6 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnOJ136jDOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e90113-0a07-4fc6-de54-91a773a0b927"
      },
      "source": [
        "#check some sample bluescores with different paramaters\r\n",
        "#1. put more emphasis on unigrams (0.8) and a little bit on bi-grams (0.1) and smoothing function of 7 and no auto_reweigh - Option to re-normalize the weights uniformly.\r\n",
        "bleuscore_dataframe(sample_test_pairs[[\"references\",\"predictions\"]], weights=(0.8,0.1,0,0),smoothFn=7)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5372258789011285"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dZvy9o2zBjG",
        "outputId": "fee78e57-fe14-47b6-a185-b5e3d15b2ecb"
      },
      "source": [
        "#2. put more emphasis on un-igrams (0.8)  and smoothing function of 7 and with auto_reweigh\r\n",
        "bleuscore_dataframe(sample_test_pairs[[\"references\",\"predictions\"]], weights=(0.8,0,0,0),smoothFn=7, auto_reweigh=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6187342840449511"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnbqoA1tzqYI",
        "outputId": "2e6d84e6-1eac-43fc-87d8-d8896e99aa37"
      },
      "source": [
        "#3. put just enough emphasis on un-igrams (0.58)  and smoothing function of 7 and with auto_reweigh\r\n",
        "bleuscore_dataframe(sample_test_pairs[[\"references\",\"predictions\"]], weights=(0.58,0,0,0),smoothFn=7, auto_reweigh=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7060582487467826"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gjvNiJz0CjP",
        "outputId": "ed6943aa-2e1e-406c-a535-105b65e4e32a"
      },
      "source": [
        "#4. put just enough emphasis on un-igrams (0.61)  with no smoothing function and with auto_reweigh\r\n",
        "bleuscore_dataframe(sample_test_pairs[[\"references\",\"predictions\"]], weights=(0.61,0,0,0),smoothFn=None, auto_reweigh=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44204042731236304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90IfadHSyPQ4",
        "outputId": "895a3c32-1349-4c36-a3fc-cdcd7d8f59d9"
      },
      "source": [
        "#5. spread empahasis accross 5-grams  with smoothing function of 7 and with auto_reweigh\r\n",
        "bleuscore_dataframe(sample_test_pairs[[\"references\",\"predictions\"]], weights=(1./5., 1./5., 1./5., 1./5., 1./5.),smoothFn=7,  auto_reweigh=True) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1113540603137754"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}